{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkQcypU9eJyU",
    "outputId": "7e15bcd7-47c1-4626-f8ef-7c99d904cadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-30 09:50:34--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60999314 (58M) [application/x-httpd-php]\n",
      "Saving to: ‘UCI HAR Dataset.zip’\n",
      "\n",
      "UCI HAR Dataset.zip 100%[===================>]  58.17M  31.7MB/s    in 1.8s    \n",
      "\n",
      "2021-09-30 09:50:38 (31.7 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]\n",
      "\n",
      "Archive:  UCI HAR Dataset.zip\n",
      "   creating: UCI HAR Dataset/\n",
      "  inflating: UCI HAR Dataset/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/UCI HAR Dataset/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/._.DS_Store  \n",
      "  inflating: UCI HAR Dataset/activity_labels.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._activity_labels.txt  \n",
      "  inflating: UCI HAR Dataset/features.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._features.txt  \n",
      "  inflating: UCI HAR Dataset/features_info.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._features_info.txt  \n",
      "  inflating: UCI HAR Dataset/README.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._README.txt  \n",
      "   creating: UCI HAR Dataset/test/\n",
      "   creating: UCI HAR Dataset/test/Inertial Signals/\n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
      "   creating: __MACOSX/UCI HAR Dataset/test/\n",
      "   creating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._Inertial Signals  \n",
      "  inflating: UCI HAR Dataset/test/subject_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._subject_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/X_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._X_test.txt  \n",
      "  inflating: UCI HAR Dataset/test/y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/test/._y_test.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._test  \n",
      "   creating: UCI HAR Dataset/train/\n",
      "   creating: UCI HAR Dataset/train/Inertial Signals/\n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
      "   creating: __MACOSX/UCI HAR Dataset/train/\n",
      "   creating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/\n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._Inertial Signals  \n",
      "  inflating: UCI HAR Dataset/train/subject_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._subject_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/X_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._X_train.txt  \n",
      "  inflating: UCI HAR Dataset/train/y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/train/._y_train.txt  \n",
      "  inflating: __MACOSX/UCI HAR Dataset/._train  \n",
      "  inflating: __MACOSX/._UCI HAR Dataset  \n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
    "!unzip \"UCI HAR Dataset.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qoX7HzvSYFEW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "arDFs0OBXuLH"
   },
   "outputs": [],
   "source": [
    "from tensorflow import reshape\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "foCSW3coaorZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Conv1D, BatchNormalization, MaxPool1D, Dropout, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FWFPbchEX_8t"
   },
   "outputs": [],
   "source": [
    "def format_data_x(datafile):\n",
    "    x_data = None\n",
    "    for item in datafile:\n",
    "        item_data = np.loadtxt(item, dtype=np.float)\n",
    "        if x_data is None:\n",
    "            x_data = np.zeros((len(item_data), 1))\n",
    "        x_data = np.hstack((x_data, item_data))\n",
    "    x_data = x_data[:, 1:]\n",
    "    print(x_data.shape)\n",
    "    X = None\n",
    "    for i in range(len(x_data)):\n",
    "        row = np.asarray(x_data[i, :])\n",
    "        row = row.reshape(9, 128).T\n",
    "        if X is None:\n",
    "            X = np.zeros((len(x_data), 128, 9))\n",
    "        X[i] = row\n",
    "    print(X.shape)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z2JFKTJmYBmg"
   },
   "outputs": [],
   "source": [
    "def format_data_y(datafile):\n",
    "    data = np.loadtxt(datafile, dtype=np.int) - 1\n",
    "    YY = np.eye(6,dtype=np.int)[data]\n",
    "    return YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PsZEGwO9O0KI"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import os\n",
    "    if os.path.isfile('data/data_har.npz') == True:\n",
    "        data = np.load('data/data_har.npz')\n",
    "        X_train = data['X_train']\n",
    "        Y_train = data['Y_train']\n",
    "        X_test = data['X_test']\n",
    "        Y_test = data['Y_test']\n",
    "    else:\n",
    "        # This for processing the dataset from scratch\n",
    "        # After downloading the dataset, put it to somewhere that str_folder can find\n",
    "        str_folder = './UCI HAR Dataset/'\n",
    "        INPUT_SIGNAL_TYPES = [\n",
    "            \"body_acc_x_\",\n",
    "            \"body_acc_y_\",\n",
    "            \"body_acc_z_\",\n",
    "            \"body_gyro_x_\",\n",
    "            \"body_gyro_y_\",\n",
    "            \"body_gyro_z_\",\n",
    "            \"total_acc_x_\",\n",
    "            \"total_acc_y_\",\n",
    "            \"total_acc_z_\"\n",
    "        ]\n",
    "\n",
    "        str_train_files = [str_folder + 'train/' + 'Inertial Signals/' + item + 'train.txt' for item in\n",
    "                           INPUT_SIGNAL_TYPES]\n",
    "        str_test_files = [str_folder + 'test/' + 'Inertial Signals/' + item + 'test.txt' for item in INPUT_SIGNAL_TYPES]\n",
    "        str_train_y = str_folder + 'train/y_train.txt'\n",
    "        str_test_y = str_folder + 'test/y_test.txt'\n",
    "\n",
    "        X_train = format_data_x(str_train_files)\n",
    "        X_test = format_data_x(str_test_files)\n",
    "        Y_train = format_data_y(str_train_y)\n",
    "        Y_test = format_data_y(str_test_y)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umutVCfjWk5q",
    "outputId": "9af938b5-1cec-480b-f379-2a71352f6a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 1152)\n",
      "(7352, 128, 9)\n",
      "(2947, 1152)\n",
      "(2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itePe80H2rCX"
   },
   "source": [
    "## Client as Data Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TaTH6hM-3Ra9"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sGJbHpgm2qnu"
   },
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iNG3D1lq2yTM"
   },
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, Y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRCGiCK43MkQ"
   },
   "source": [
    "## Batching Data into tensorflow formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bLyxM8Nc3L9X"
   },
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wefel4r83nI3"
   },
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnJ8c-_k5bVg",
    "outputId": "7f3ea4cf-3fe7-44b1-9dfd-b1a1b2db44da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_1': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_10': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_2': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_3': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_4': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_5': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_6': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_7': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_8': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>,\n",
       " 'client_9': <BatchDataset shapes: ((None, 128, 9), (None, 6)), types: (tf.float64, tf.int64)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk3F-ZyS3yev"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-3FmFFi30D5",
    "outputId": "6b75ca82-633b-491c-d0cc-0b5648027d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 128, 32)           608       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 64, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 64, 64)            4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 32, 128)           16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 226,786\n",
      "Trainable params: 226,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_keras_model():\n",
    "\n",
    "    # Create keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st convolution layer\n",
    "    model.add(Input(shape=(128,9)))\n",
    "\n",
    "    model.add(Conv1D(filters=32,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu))\n",
    "    model.add(MaxPool1D(pool_size=4,strides=2,padding='same'))\n",
    "\n",
    "    model.add(Conv1D(filters=64,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu))\n",
    "    model.add(MaxPool1D(pool_size=4,strides=2,padding='same'))\n",
    "\n",
    "    model.add(Conv1D(filters=128,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu))\n",
    "    model.add(MaxPool1D(pool_size=4,strides=2,padding='same'))\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    #model.compile(loss=SparseCategoricalCrossentropy(),\n",
    "    #              optimizer=SGD(learning_rate=0.02),\n",
    "    #              metrics=[SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Summary model\n",
    "keras_model = create_keras_model()\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Lc_lvOtf4UvF"
   },
   "outputs": [],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 50\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIkCVC9O4bXC"
   },
   "source": [
    "## Federated Learning Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EA8OESOb4df0"
   },
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xId_Ru_84eXB",
    "outputId": "1bdb60f4-c23d-452a-d20b-94a3ecafdfb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 59.688% | global_loss: 1.705342411994934\n",
      "comm_round: 1 | global_acc: 71.157% | global_loss: 1.5064157247543335\n",
      "comm_round: 2 | global_acc: 73.736% | global_loss: 1.4260972738265991\n",
      "comm_round: 3 | global_acc: 77.367% | global_loss: 1.3703622817993164\n",
      "comm_round: 4 | global_acc: 78.079% | global_loss: 1.352492094039917\n",
      "comm_round: 5 | global_acc: 78.385% | global_loss: 1.321596384048462\n",
      "comm_round: 6 | global_acc: 78.724% | global_loss: 1.3046586513519287\n",
      "comm_round: 7 | global_acc: 80.794% | global_loss: 1.2948840856552124\n",
      "comm_round: 8 | global_acc: 80.048% | global_loss: 1.2896400690078735\n",
      "comm_round: 9 | global_acc: 79.912% | global_loss: 1.2874852418899536\n",
      "comm_round: 10 | global_acc: 80.692% | global_loss: 1.2608797550201416\n",
      "comm_round: 11 | global_acc: 82.525% | global_loss: 1.2512247562408447\n",
      "comm_round: 12 | global_acc: 84.798% | global_loss: 1.2484595775604248\n",
      "comm_round: 13 | global_acc: 84.730% | global_loss: 1.2345250844955444\n",
      "comm_round: 14 | global_acc: 84.900% | global_loss: 1.227367639541626\n",
      "comm_round: 15 | global_acc: 85.850% | global_loss: 1.2237706184387207\n",
      "comm_round: 16 | global_acc: 86.800% | global_loss: 1.2124011516571045\n",
      "comm_round: 17 | global_acc: 87.445% | global_loss: 1.207483172416687\n",
      "comm_round: 18 | global_acc: 87.716% | global_loss: 1.203365445137024\n",
      "comm_round: 19 | global_acc: 86.359% | global_loss: 1.2080559730529785\n",
      "comm_round: 20 | global_acc: 87.072% | global_loss: 1.2021279335021973\n",
      "comm_round: 21 | global_acc: 88.599% | global_loss: 1.1940921545028687\n",
      "comm_round: 22 | global_acc: 88.157% | global_loss: 1.1890302896499634\n",
      "comm_round: 23 | global_acc: 88.734% | global_loss: 1.1879229545593262\n",
      "comm_round: 24 | global_acc: 88.531% | global_loss: 1.1844247579574585\n",
      "comm_round: 25 | global_acc: 88.599% | global_loss: 1.182833194732666\n",
      "comm_round: 26 | global_acc: 88.972% | global_loss: 1.1820839643478394\n",
      "comm_round: 27 | global_acc: 89.175% | global_loss: 1.1800613403320312\n",
      "comm_round: 28 | global_acc: 89.617% | global_loss: 1.1790012121200562\n",
      "comm_round: 29 | global_acc: 89.515% | global_loss: 1.1787357330322266\n",
      "comm_round: 30 | global_acc: 89.718% | global_loss: 1.1778868436813354\n",
      "comm_round: 31 | global_acc: 89.617% | global_loss: 1.1760977506637573\n",
      "comm_round: 32 | global_acc: 88.870% | global_loss: 1.175614356994629\n",
      "comm_round: 33 | global_acc: 89.413% | global_loss: 1.1748744249343872\n",
      "comm_round: 34 | global_acc: 89.311% | global_loss: 1.175252079963684\n",
      "comm_round: 35 | global_acc: 88.531% | global_loss: 1.1757200956344604\n",
      "comm_round: 36 | global_acc: 89.956% | global_loss: 1.1721891164779663\n",
      "comm_round: 37 | global_acc: 89.922% | global_loss: 1.1714316606521606\n",
      "comm_round: 38 | global_acc: 89.006% | global_loss: 1.171860694885254\n",
      "comm_round: 39 | global_acc: 90.126% | global_loss: 1.1694399118423462\n",
      "comm_round: 40 | global_acc: 89.108% | global_loss: 1.1712945699691772\n",
      "comm_round: 41 | global_acc: 89.311% | global_loss: 1.1681209802627563\n",
      "comm_round: 42 | global_acc: 89.006% | global_loss: 1.1687591075897217\n",
      "comm_round: 43 | global_acc: 89.583% | global_loss: 1.1692818403244019\n",
      "comm_round: 44 | global_acc: 89.311% | global_loss: 1.1674014329910278\n",
      "comm_round: 45 | global_acc: 89.243% | global_loss: 1.1683568954467773\n",
      "comm_round: 46 | global_acc: 89.209% | global_loss: 1.167441725730896\n",
      "comm_round: 47 | global_acc: 89.820% | global_loss: 1.1661823987960815\n",
      "comm_round: 48 | global_acc: 89.752% | global_loss: 1.1660839319229126\n",
      "comm_round: 49 | global_acc: 89.311% | global_loss: 1.165130853652954\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "global_model = create_keras_model()\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        local_model = create_keras_model()\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
